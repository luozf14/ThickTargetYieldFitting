{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential cross section fitting using BRICK \n",
    "## $^{12}\\rm{C}(^{12}C, \\alpha_{0})^{20}\\rm{Ne}$ \n",
    "\n",
    "## Steps\n",
    "* Construct a Python object that enables the calling of AZURE2 with an arbitrary input vector $\\theta$.\n",
    "* Set up priors for each of the sampled parameters and a corresponding prior ($\\ln\\Pi$) function.\n",
    "* Construct a global function that reads a SRIM file, input energy, and return the -dE/dx.\n",
    "* Construct the yield function.\n",
    "* Set up a likelihood ($\\ln\\mathcal{L}$) function.\n",
    "* Construct a posterior ($\\ln\\mathcal{P}$) function from the likelihood and prior functions.\n",
    "* Prepare the MCMC walker and sample the posterior ($\\ln\\mathcal{P}$) function with emcee.\n",
    "* Preview the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad, quadrature\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from brick.azr import AZR\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "* Construct a Python object that enables the calling of AZURE2 with an arbitrary input vector $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict processes to one thread only. Good for AZURE2\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Construct an object that calls AZURE2 and takes input 'a0_3.00MeV.arz'\n",
    "azr = AZR('a0.arz')\n",
    "\n",
    "# How many parameters are going to be sampled?\n",
    "nd = azr.config.nd + 1\n",
    "print(f'There are {nd} sampled parameters.\\n')\n",
    "\n",
    "# Get the initial input parameters\n",
    "# theta0 = azr.config.get_input_values()\n",
    "\n",
    "theta0=[]\n",
    "# pattern = r'G\\s*=\\s*(\\d+\\.\\d+)\\s*(keV|eV|meV)'\n",
    "# with open('output/parameters.out', 'r') as file:\n",
    "#     for line in file:\n",
    "#         match = re.search(pattern, line)\n",
    "#         if match:\n",
    "#             value = float(match.group(1))\n",
    "#             unit = match.group(2)\n",
    "#             # Convert units to eV\n",
    "#             if unit == 'keV':\n",
    "#                 value *= 1e3\n",
    "#             elif unit == 'meV':\n",
    "#                 value *=1e-3\n",
    "#             theta0.append(value)\n",
    "for i in range(nd-1):\n",
    "    theta0.append(0.1*1E6)\n",
    "\n",
    "theta0.append(1E5)\n",
    "print(f'The initial input parameters are:\\n {theta0}\\n')\n",
    "\n",
    "# What labels have been assigned to those parameters?\n",
    "labels = azr.config.labels\n",
    "labels.append('$N_{v}$')\n",
    "print(f'The labels of the input parameters are:\\n {labels}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data files\n",
    "data_files_list = os.listdir('data/')\n",
    "data_files_list.sort()\n",
    "print(data_files_list)\n",
    "num_energies = len(data_files_list)\n",
    "energies = [float(filename.replace('.txt', '')) for filename in data_files_list]\n",
    "energies = np.array(energies)\n",
    "data = []\n",
    "angle_lab = []\n",
    "yield_lab = []\n",
    "yield_err = []\n",
    "for i in range(num_energies):\n",
    "    temp = np.loadtxt(\"data/\"+data_files_list[i])\n",
    "    temp[:,2]=temp[:,2]*1E-10 # Scale factor of the yield\n",
    "    temp[:,3]=temp[:,3]*1E-10 # Scale factor of the yield's error\n",
    "    data.append(temp)\n",
    "    angle_lab.append(temp[:,1])\n",
    "    yield_lab.append(temp[:,2])\n",
    "    yield_err.append(temp[:,3])\n",
    "num_angles = len(angle_lab[0])\n",
    "angle_lab = np.asarray(angle_lab, dtype='float32').T\n",
    "yield_lab = np.asarray(yield_lab, dtype='float32').T\n",
    "yield_err = np.asarray(yield_err, dtype='float32').T\n",
    "print('shape of yield_lab is', yield_lab.shape)\n",
    "\n",
    "# Take a look at the data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for i in range(num_energies):\n",
    "    ax.errorbar(angle_lab[:,i], yield_lab[:,i], yerr=yield_err[:,i], linestyle='', capsize=2, fmt=\"o\", label=data_files_list[i])\n",
    "    # ax.scatter(angle_lab[i], yield_lab[i])\n",
    "ax.legend()\n",
    "ax.set_xlim(100, 140)\n",
    "ax.set_xlabel(r'$\\theta$ (deg, lab)')\n",
    "ax.set_ylabel(r'Yields (arb. units)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "* Set up priors for each of the sampled parameters and a corresponding prior ($\\ln\\Pi$) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors = []\n",
    "# for i in range(nd-1):\n",
    "#    if (i == 0) or (i==4) or (i==8) or (i==12) or (i==16) or (i==20) or (i==25) or (i==30) or (i==35) or (i==40) or (i==45) or (i==50) or (i==55) or (i==60) or (i==65):\n",
    "#       priors.append(stats.norm(theta0[i], 0.1*theta0[i]/2.355))\n",
    "#    else:\n",
    "#       priors.append(stats.uniform(0, 1E5))\n",
    "# priors.append(stats.uniform(1E4,1E6))\n",
    "\n",
    "\n",
    "# priors = [\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,1}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,2}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,3}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,1}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,2}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,3}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,1}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,2}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,3}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,1}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,2}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,3}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,1}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,2}^{(+0.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,3}^{(+0.0)}$'\n",
    "\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,1}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,2}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,3}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,4}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,1}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,2}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,3}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,4}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,1}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,2}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,3}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,4}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,1}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,2}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,3}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,4}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,1}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,2}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,3}^{(+2.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,4}^{(+2.0)}$'\n",
    "\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,1}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,2}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,3}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{1,4}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,1}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,2}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,3}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{2,4}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,1}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,2}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,3}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{3,4}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,1}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,2}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,3}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{4,4}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,1}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,2}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,3}^{(+4.0)}$'\n",
    "#     stats.uniform(0, 1E5), #'$\\\\Gamma_{5,4}^{(+4.0)}$'\n",
    "\n",
    "#     stats.uniform(1E4,1E6) # N_{v}\n",
    "# ]\n",
    "\n",
    "priors = []\n",
    "for i in range(len(theta0)-1):\n",
    "    priors.append(stats.norm(0.1*1E6, 0.1*1E6))\n",
    "priors.append(stats.uniform(1E3,1E7))\n",
    "\n",
    "# mask = np.array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]) # Widths of the Gaussians can be different.\n",
    "# priors = [stats.norm(mu, abs(mu) * mask[i]) for i, mu in enumerate(theta0)]\n",
    "\n",
    "def lnPi(theta):\n",
    "    return np.sum([pi.logpdf(t) for (pi, t) in zip(priors, theta)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "* Construct a global function that reads a SRIM file, input energy, and return the -dE/dx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the SRIM table\n",
    "srim_file = 'srim/CarbonInCarbon.txt'\n",
    "srim_table = []\n",
    "with open(srim_file, 'r') as file:\n",
    "    for line in file:\n",
    "        cols = line.split()\n",
    "        srim_table.append((float(cols[0]), float(cols[1]))) # (MeV, MeV/mm)\n",
    "srim_table=np.array(srim_table)\n",
    "\n",
    "# Construct the interpolation\n",
    "srim_interp = interp1d(srim_table[:,0], srim_table[:,1],kind='cubic')\n",
    "new_x = np.arange(0.1,50,0.1)\n",
    "plt.figure()\n",
    "plt.plot(srim_table[:,0], srim_table[:,1], 'o', label='SRIM table')\n",
    "plt.plot(new_x, srim_interp(new_x), '-', label='Interpolated -dE/dX')\n",
    "plt.xlabel('E (MeV)')\n",
    "plt.ylabel('-dE/dx (MeV/mm)')\n",
    "plt.title('Stopping power of carbon in carbon')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "* Construct the yield function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y_{\\rm lab}(\\theta_{\\rm lab}; E_0)=N_{V}\\int_{0}^{E_0}\\left[\\sigma_{\\rm lab}(\\theta_{\\rm lab}; E)/-\\frac{{\\rm d} E}{{\\rm d} x}\\right]{\\rm d} E$$\n",
    "\n",
    "where $\\sigma_{\\rm lab}(\\theta_{\\rm lab})=\\frac{(1+\\gamma^2+2\\gamma\\cos\\theta_{\\rm c})^{1.5}}{1+\\gamma\\cos\\theta_{\\rm c}}\\sigma_{\\rm c}(\\theta_{\\rm c})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COM to Lab\n",
    "m_a = 12\n",
    "m_A = 12\n",
    "m_b = 4\n",
    "m_B = 20\n",
    "Q_value=4.62\n",
    "# gamma = np.sqrt((m_a*m_b)/(m_A*m_B)*(m_b+m_B)/(m_a+m_A)*E_COM[0]/(E_COM[0]+Q))\n",
    "def yield_angle(theta, angle_index):\n",
    "    output = azr.extrapolate(theta[:-1],[angle_index])[0]\n",
    "    energy_c = output[:,0]\n",
    "    angle_c = output[:,2]\n",
    "    sigma_c = output[:,3]\n",
    "    energy_l=(m_a+m_A)/m_A*energy_c\n",
    "    gamma_local = np.sqrt((m_a*m_b)/(m_A*m_B)*(m_b+m_B)/(m_a+m_A)*energy_c/(energy_c+Q_value))\n",
    "    sigma_l = (1+gamma_local**2.+2.*gamma_local*np.cos(angle_c/180.*np.pi))**(3./2.)/(1+gamma_local*np.cos(angle_c/180.*np.pi))*sigma_c\n",
    "    # a_l=np.arccos((gamma_local+np.cos(angle_c/180*np.pi))/np.sqrt(1+gamma_local**2+2*gamma_local*np.cos(angle_c/180*np.pi)))\n",
    "    sigma_interp = interp1d(energy_l, sigma_l, kind='linear')\n",
    "    def integrand(E):\n",
    "        return sigma_interp(E)/srim_interp(E)\n",
    "    res = np.zeros(num_energies)\n",
    "    res[0] = quad(integrand, 0.5, energies[0]*(m_a+m_A)/m_A)[0] # Integrate from E = 0.5 to 6 MeV\n",
    "    for i in range(num_energies):\n",
    "        if(i==0):\n",
    "            continue\n",
    "        else:\n",
    "            res[i] = res[i-1]+quad(integrand, energies[i-1]*(m_a+m_A)/m_A, energies[i]*(m_a+m_A)/m_A)[0] # Integrate from E_{i-1} to E_{i}\n",
    "    return res * theta[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "* Set up a likelihood ($\\ln\\mathcal{L}$) function.\n",
    "\n",
    "We'll assume that the data are random draws from uncorrelated Gaussian distributions centered at the $R$-matrix predictions. So, we have\n",
    "$$\n",
    "\\mathcal{L} = \\prod_i \\frac{1}{\\sqrt{2\\pi}\\sigma_i} e^{-\\frac{1}{2}\\left(\\frac{y_i-\\mu_i}{\\sigma_i}\\right)^2}~,\n",
    "$$\n",
    "where $y_i$ and $\\sigma_i$ are the data points and uncertainties, respectively, and $\\mu_i$ is the physical model prediction.\n",
    "\n",
    "We work with $\\ln{\\mathcal{L}}$ to avoid the numerical difficulties of dealing with extremely small values. The product then becomes a sum,\n",
    "$$\n",
    "\\ln{\\mathcal{L}} = \\sum_i \\left[-\\ln{(\\sqrt{2\\pi}\\sigma_i)} - \\frac{1}{2}\\left(\\frac{y_i-\\mu_i}{\\sigma_i}\\right)^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnL(theta):\n",
    "    mu=[]\n",
    "    for i in range(num_angles):\n",
    "        mu.append(yield_angle(theta, i))\n",
    "    mu = np.array(mu, dtype='float32')\n",
    "    temp_sum = np.zeros(num_energies)\n",
    "    for i in range(num_energies):\n",
    "        temp_sum[i]=np.sum(-np.log(np.sqrt(2*np.pi)*yield_err[:,i]) - 0.5*((yield_lab[:,i] - mu[:,i])/yield_err[:,i])**2)\n",
    "    return np.sum(temp_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "* Construct a posterior ($\\ln\\mathcal{P}$) function from the likelihood and prior functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnP(theta):\n",
    "    lnpi = lnPi( theta )                        # Calculate the prior\n",
    "    if not np.isfinite( lnpi ): \n",
    "        return -np.inf  # If the prior is not finite, return -inf\n",
    "    lnl = lnL( theta )                          # Calculate the likelihood\n",
    "    return lnl + lnpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "* Prepare the MCMC walker and sample the posterior ($\\ln\\mathcal{P}$) function with emcee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = 2 * nd  # number of walkers = 2 * number of sampled parameters\n",
    "\n",
    "# Each walker needs its own starting position. We'll take normally distributed\n",
    "# random values centered at theta0.\n",
    "p0 = np.zeros((nw, nd))\n",
    "# mask = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]) # Widths of the Gaussians can be different.\n",
    "mask = np.zeros(nd)\n",
    "for i in range(nd):\n",
    "    mask[i] = 0.01\n",
    "for i in range(nw):\n",
    "    mu = theta0\n",
    "    sig = np.abs(theta0) * mask\n",
    "    p0[i, :] = stats.norm(mu, sig).rvs()\n",
    "\n",
    "\n",
    "# We'll store the chain in chain.h5. (See emcee Backends documentation.)\n",
    "backend = emcee.backends.HDFBackend(\"chain.h5\")\n",
    "backend.reset(nw, nd)\n",
    "\n",
    "nsteps = 10  # How many saved steps should each walker make?\n",
    "nthin = 1  # How often should the walkr save a step? n_total = nsteps * nthin\n",
    "nprocs = nw  # How many Python processes do you want to allocate?\n",
    "\n",
    "# For a stretch move\n",
    "move = emcee.moves.StretchMove()\n",
    "\n",
    "# emcee allows the user to specify the way the ensemble generates proposals.\n",
    "with Pool(processes=24) as pool:\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nw, nd, lnP, pool=pool, backend=backend, moves=[move]\n",
    "    )\n",
    "    state = sampler.run_mcmc(p0, nsteps, thin_by=nthin, progress=True, tune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Posterior function evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the probability\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "lnp = backend.get_log_prob(discard=1)\n",
    "ax.plot(lnp)\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('ln( P )')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Parameters evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the samples\n",
    "nb=90 # discard the first nb samples\n",
    "flat_samples = sampler.get_chain(flat=True, discard=nb)\n",
    "\n",
    "# Plot the parameters\n",
    "fig, ax = plt.subplots( int(nd / 2)+1, 2, figsize=( 15, 30 ) )\n",
    "\n",
    "means, lows, highs = [], [], []\n",
    "final_results = np.zeros(nd)\n",
    "for i in range( nd ):\n",
    "\n",
    "        mean = np.percentile( flat_samples[:,i], 50 )\n",
    "        low = np.percentile( flat_samples[:,i], 16 )\n",
    "        high = np.percentile( flat_samples[:,i], 84 )\n",
    "    \n",
    "        ax[i//2, i%2].set_title( labels[i] )\n",
    "        ax[i//2, i%2].set_xlabel( \"Step\" )\n",
    "\n",
    "        ax[i//2, i%2].plot( flat_samples[:,i], color=\"tab:blue\" )\n",
    "        ax[i//2, i%2].axhline( mean, color=\"tab:red\", linestyle=\"-\", lw=4 )\n",
    "        ax[i//2, i%2].axhline( low, color=\"tab:red\", linestyle=\"--\", lw=2 )\n",
    "        ax[i//2, i%2].axhline( high, color=\"tab:red\", linestyle=\"--\", lw=2 )\n",
    "\n",
    "        ax[i//2, i%2].set_ylabel( \"Value\" )\n",
    "\n",
    "        means.append( mean )\n",
    "        lows.append( low )\n",
    "        highs.append( high )\n",
    "\n",
    "# Save parameters to file\n",
    "with open( \"results/best-emcee.txt\", \"w\" ) as f:\n",
    "    for i in range( nd ):\n",
    "        f.write( f\"{azr.config.labels[i]}: {means[i]:.5f} + {highs[i]-means[i]:.5f} - {means[i]-lows[i]:.5f}\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = corner.corner(\n",
    "#     flat_samples, labels=labels, truths=means, quantiles=[0.16, 0.5, 0.84], title_fmt=\".4f\", fill_contours=True,\n",
    "#     show_titles=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_factor(theta):\n",
    "    output = np.array(azr.extrapolate(theta[:-1],[24])[0])\n",
    "    return np.array([output[:,0],output[:,3], output[:,4]])\n",
    "\n",
    "\n",
    "def process_parallel():\n",
    "    # Shuffle and take last 100 samples\n",
    "    np.random.shuffle(flat_samples)\n",
    "    samples_to_process = flat_samples[-100:]\n",
    "    \n",
    "    # Create a pool of workers\n",
    "    with Pool(processes=24) as pool:\n",
    "        # Map the function to the samples and show progress\n",
    "        buckets = list(tqdm(\n",
    "            pool.imap(s_factor, samples_to_process),\n",
    "            total=len(samples_to_process)\n",
    "        ))\n",
    "    \n",
    "    return buckets\n",
    "buckets = process_parallel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reampled results and compared it to the data\n",
    "buckets=np.array(buckets)\n",
    "\n",
    "mean = np.percentile(buckets, 50, axis=0 ).T\n",
    "low = np.percentile(buckets, 16, axis=0).T\n",
    "high = np.percentile(buckets, 84, axis=0).T\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=( 15, 5 ))\n",
    "ax[0].set_xlim(0, 5)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Ecm (MeV, COM)')\n",
    "ax[0].set_ylabel(r'Cross section (barn)')\n",
    "ax[0].plot(mean[20:,0], mean[20:,1], label=\"Extrapolation\", color=\"red\")\n",
    "ax[0].fill_between(mean[20:,0], low[20:,1], high[20:,1], color=\"red\", alpha=0.5 )\n",
    "\n",
    "ax[1].set_xlim(0, 5)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Ecm (MeV, COM)')\n",
    "ax[1].set_ylabel(r'S-factor (MeV b)')\n",
    "ax[1].plot(mean[:,0], mean[:,2], label=\"Extrapolation\", color=\"red\")\n",
    "ax[1].fill_between(mean[:,0], low[:,2], high[:,2], color=\"red\", alpha=0.5 )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
